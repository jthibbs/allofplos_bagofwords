{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47002"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################################################################################################\n",
    "    #####################################################################################################\n",
    "             ############  Text proprocessing, article by article (bag of words)  ############\n",
    "    #####################################################################################################\n",
    "###############################################################################################################\n",
    "\n",
    "    ################################################################################################\n",
    "        #### For more notes on these methods see file 'MAIN_text_preprocessing_w_allofplos' ####\n",
    "    ################################################################################################\n",
    "\n",
    "#####################################\n",
    " ##### TO DO FIRST TO MAKE THIS WORK: \n",
    "#####################################\n",
    "\n",
    "########## 1. Change all xml files into txt: \n",
    " # *** Go to the command prompt and type in:\n",
    "   # A. cd\\\n",
    "   # B. cd\\Users\\jthibodeaux\\python\\Lib\\site-packages\\allofplos\\allofplos_text\n",
    "   # C. ren *.xml *.txt\n",
    "\n",
    "\n",
    "############################################################\n",
    "########## NOTE: This code reaches memory failure after about 70k articles for title and abstract only. \n",
    "  ## I can't hold 100k cols with 250k rows; I don't even think I'd be able to hold 50k cols with 250k rows.\n",
    "  ## SO: Break allofplos corpus into 1/4th (even smaller fractions if you're doing the entire text instead of title/abstract).\n",
    "       # OR change step 5: the function that create the sum of all words across the corpus. When I tried a loop of summing the function took about 3 times as long, so doing it in 4 iterations seemed like a better tactic.\n",
    "  ## If you use the entire text of each file, do 10-15 iterations (I did 20k files before with full text, 35k was too much).\n",
    "############################################################\n",
    "\n",
    "\n",
    "#####################################\n",
    " ##### The below opens / reads all of the text files from a folder into Python.\n",
    "#####################################\n",
    "import os\n",
    "import pandas as pd\n",
    "allLines = []\n",
    "path = 'python/Lib/site-packages/allofplos/allofplos_text_5/'\n",
    "  # NOTE: This means the 'allofplos_text' folder has to be a child folder of the folder this jupyter notebook is in.\n",
    "fileList = os.listdir(path)\n",
    "for file in fileList:\n",
    "   file = open(os.path.join('python/Lib/site-packages/allofplos/allofplos_text_5/'+ file), 'r', encoding='utf-8')\n",
    "   allLines.append(file.read())\n",
    "\n",
    "\n",
    "####### This changes the file list into dois by taking off the .txt:\n",
    "fileList = [x[:-4] for x in fileList]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    " ##### Specify how many chunks you want your corpus broken into \n",
    "    # NOTE: This is to prevent memory failure AND to provide datasets with few columns (and thus a lower storage size).\n",
    "       # It's likely a better to create a large number of smaller csvs (i.e. large number of 'chunk_num' with no NaNs for creating the 'Related articles' for new articles: these NaNs make the total files way larger then need be. Then create the column sums across all of these CSVs and save.\n",
    "           # This no NaN data is done below if you use 'dataframe_collection' instead of 'modified_collection'\n",
    "#####################################\n",
    "chunk_num = 18\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Make sure all files were imported by checking length:\n",
    "len(allLines)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    " ##### BOTH OF THE BELOW FILTERS BY RESEARCH-ARTICLES *AND* TAKES OUT HTML LANGUAGE\n",
    "   ## NOTE: One reason I only get research articles is because NON-research articles (or some subset of them) have some weird issue with them when tokenizing\n",
    "#################################################\n",
    "#################################################\n",
    " ## Time difference:\n",
    "    ##  #1 (ALL WORDS) takes about 3 articles per second. #2 (title & abstract only) takes about 30 articles per second. \n",
    "#################################################\n",
    "\n",
    "\n",
    "#################################################\n",
    "##### 1. The below use ALL WORDS from the article:\n",
    "#################################################\n",
    "\n",
    "############################################################################ Disabled for now\n",
    "#import re\n",
    "#from bs4 import BeautifulSoup\n",
    "#full_corpus = []\n",
    "#research_article_list = []\n",
    "#for i in range(0, len(allLines)):\n",
    "#    if len(re.findall('article-type=\"research-article', allLines[i])) > 0:    \n",
    "#        soup = BeautifulSoup(allLines[i], \"xml\")\n",
    "#        text = soup.get_text(strip=False)\n",
    "#        full_corpus.append(text)\n",
    "#        research_article_list.append(fileList[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################################\n",
    "##### 2. The below removes all text except title and abstract:\n",
    "#################################################\n",
    "\n",
    "##################\n",
    "######## This allows you to get the title and abstract: \n",
    "#### You use split to split the data by a particular string, then only use the string beforehand.\n",
    "  # E.g. see: https://stackoverflow.com/questions/27387415/how-would-i-get-everything-before-a-in-a-string-python\n",
    "##Title\n",
    "   # Start: <title-group>\n",
    "   # End: <alt-title alt-title-type\n",
    "##Abstract_1:\n",
    "   # Start: </permissions>\n",
    "   # End: </abstract>\n",
    "      # Also, add on a second group of text if there's over 1 </abstract> after the </permissions>\n",
    "##Abstract_2:\n",
    "   # Start: </permissions>\n",
    "   # End: <abstract abstract-type=\"summary\">\n",
    "##################\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "full_corpus = []\n",
    "research_article_list = []\n",
    "for i in range(0, len(allLines)):\n",
    "    if len(re.findall('article-type=\"research-article', allLines[i])) > 0:    \n",
    "        text = str(allLines[i])\n",
    "        ## Titles look to be called similarly across allofplos (abstracts are more difficult, see below)\n",
    "        title = text.split('<alt-title alt-title-type')[0]\n",
    "        title = title.split('<title-group>')[1]\n",
    "        ## This is an additional way to include abstracts BUT they aren't in every article:\n",
    "        Another_abstract = 0\n",
    "        if len(re.findall('</abstract></article-meta>', text)) > 0:\n",
    "            abstract_2 = text.split('</abstract></article-meta>')[0]\n",
    "            abstract_2 = abstract_2.split('</permissions>')[1]\n",
    "            Another_abstract = Another_abstract + 1\n",
    "        else:         # This is the main way to get the abstract:\n",
    "            abstract = text.split('</permissions>')[1]\n",
    "            abstract_1 = abstract.split('</abstract>')[0]\n",
    "        ### Get rid of xml tags for the title :\n",
    "             ## NOTE: In many cases, using BeautifulSoup on the abstract portion removes a lot of the abstract so I removed it (I remove related words in the below cell under 'to_remove' such as 'abstract'):\n",
    "        soup = BeautifulSoup(title, \"xml\")\n",
    "        title = soup.get_text(strip=False)\n",
    "        ### If there's more than one </abstract> after permissions, add the part after that first </abstract> too.\n",
    "           ## NOTE: included title twice to highlight it's importance.\n",
    "        if Another_abstract > 0:\n",
    "            text = title + \" \" + title + \" \" + abstract_2\n",
    "        else:\n",
    "            if len(re.findall('</abstract>', abstract)) > 1:\n",
    "                abstract_3 = abstract.split('</abstract>')[1]\n",
    "                text = title + \" \" + title + \" \" + abstract_1 + \" \" + abstract_3\n",
    "            else:\n",
    "                text = title + \" \" + title + \" \" + abstract_1\n",
    "        full_corpus.append(text)\n",
    "        research_article_list.append(fileList[i])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Versatile Viral System for Expression and Depletion of Proteins in Mammalian Cells A Versatile Viral System for Expression and Depletion of Proteins in Mammalian Cells <abstract>\\n<p>The ability to express or deplete proteins in living cells is crucial for the study of biological processes. Viral vectors are often useful to deliver DNA constructs to cells that are difficult to transfect by other methods. Lentiviruses have the additional advantage of being able to integrate into the genomes of non-dividing mammalian cells. However, existing viral expression systems generally require different vector backbones for expression of cDNA, small hairpin RNA (shRNA) or microRNA (miRNA) and provide limited drug selection markers. Furthermore, viral backbones are often recombinogenic in bacteria, complicating the generation and maintenance of desired clones. Here, we describe a collection of 59 vectors that comprise an integrated system for constitutive or inducible expression of cDNAs, shRNAs or miRNAs, and use a wide variety of drug selection markers. These vectors are based on the Gateway technology (Invitrogen) whereby the cDNA, shRNA or miRNA of interest is cloned into an Entry vector and then recombined into a Destination vector that carries the chosen viral backbone and drug selection marker. This recombination reaction generates the desired product with &gt;95% efficiency and greatly reduces the frequency of unwanted recombination in bacteria. We generated Destination vectors for the production of both retroviruses and lentiviruses. Further, we characterized each vector for its viral titer production as well as its efficiency in expressing or depleting proteins of interest. We also generated multiple types of vectors for the production of fusion proteins and confirmed expression of each. We demonstrated the utility of these vectors in a variety of functional studies. First, we show that the FKBP12 Destabilization Domain system can be used to either express or deplete the protein of interest in mitotically-arrested cells. Also, we generate primary fibroblasts that can be induced to senesce in the presence or absence of DNA damage. Finally, we determined that both isoforms of the AT-Rich Interacting Domain 4B (ARID4B) protein could induce G1 arrest when overexpressed. As new technologies emerge, the vectors in this collection can be easily modified and adapted without the need for extensive recloning.</p>\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "full_corpus[501]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My program took 5979.725497722626 to run\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###### Combined with below, this allows you to time how fast your code runs:\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#####################################\n",
    " ##### Step 1: Create a loop to clean the articles (e.g. punctuation, upper cases) before you put it in the main method\n",
    "  ## Make the string all lower case, take out numbers, take out punctuation, take out white spaces:\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "## The following 2 could be skipped if you want to keep the stopwords:\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "new_corpus = []\n",
    "for i in range(len(full_corpus)):\n",
    "    temp = []\n",
    "    temp = full_corpus[i].lower()\n",
    "    temp = re.sub(r'\\d+', '', temp)\n",
    "    temp = temp.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    temp = temp.strip()\n",
    "    ### The following three could be skipped if you want to keep the stop words:\n",
    "    temp = word_tokenize(temp)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    temp = [i for i in temp if not i in stop_words]\n",
    "    ### Get rid of any words you think should be removed (eg uninformative common words and tags)\n",
    "    to_remove = [\"plos\", \"use\", \"using\", \"abstract\", \"sec\", \"title\", \"italic\", \"summary\", \"type\", \"toc\", \"bold\", \"relatedarticle\", \"page\", \"relatedarticletype\", \"article-title\", \"sup\", \"abstracttype\", \"reftypebibr\", \"xref\", \"contenttypepdf\", \"ref\", \"idsec\", \"summarytitl\", \"pthe\", \"yearyear\", \"abstracttypesummari\", \"volumevolum\", \"xlinktypesimplenam\", \"mixedcit\", \"rowspan\", \"xlinktypesimpl\", \"italicin\", \"extlinktypeuri\", \"titleresultstitl\", \"titlemethodstitl\", \"fpagefpag\", \"abstracttypetoc\", \"italicsital\", \"lpagelpag\", \"cdsupsup\", \"id\", \"titlemethod\", \"metaanalysi\", \"sourceplo\", \"namedcont\", \"articletitleth\", \"mimetypeimag\", \"commentdoiextlink\", \"italicmital\", \"fpageefpag\", \"abstractsec\", \"titleresultstitl\", \"findingstitl\", \"titlemethodstitl\", \"italicpitaliclt\", \"titlemethod\", \"pthese\", \"iii\", \"persongroup\", \"tgfβ\", \"ip\", \"extlink']\", \"extlink\", \"fpagefpag\", \"mimetypeimag\", \"lpagelpag\", \"cellsp']\", \"cellsp\", \"secsec\", \"titleresultstitl\", \"titlemethodstitl\", \"italicpital\", \"sectypehead\", \"link\", \"sec\", \"titleobjectivetitl\", \"tlr\", \"findingstitl\", \"ital\", \"supplement\", \"namedcont\", \"italica\", \"titlemethod\", \"methodstitl\", \"italicsital\", \"titlepurposetitl\", \"italicd\", \"titlemateri\", \"italicnital\", \"italict\", \"italicnamedcont\", \"italich\", \"titletri\", \"italiceital\", \"contenttypegenu\", \"titleresultstitl\", \"titlemethodstitl\", \"italicpital\", \"xlinktypesimpl\", \"titleobjectivetitl\", \"italicsital\", \"italiccital\", \"italiceital\", \"subtyp\", \"italicaital\", \"italicbital\", \"italicnital\", \"titlepurposetitl\", \"titlemateri\", \"italiccoliital\", \"italiclital\", \"italicpital\", \"unique\", \"name\", \"use\", \"would\", \"either\", \"introduction\"]\n",
    "    ## bad_col: Get rid of all words with 'X' in them (since beautiful soup doesn't work well on abstract):\n",
    "    bad_col_1 = re.compile(\"link:*\")                       # NOTE: This matching on link bit is new.\n",
    "    bad_col_2 = re.compile(\"abstract*\")\n",
    "    bad_col_3 = re.compile(\"title*\")\n",
    "    bad_col_4 = re.compile(\"xlink*\")\n",
    "    bad_col_5 = re.compile(\"label*\")\n",
    "    matches_1 = [i for i in temp if re.match(bad_col_1, i)]\n",
    "    matches_2 = [i for i in temp if re.match(bad_col_2, i)]\n",
    "    matches_3 = [i for i in temp if re.match(bad_col_3, i)]\n",
    "    matches_4 = [i for i in temp if re.match(bad_col_4, i)]\n",
    "    matches_5 = [i for i in temp if re.match(bad_col_5, i)]\n",
    "    to_remove.extend(matches_1)\n",
    "    to_remove.extend(matches_2)\n",
    "    to_remove.extend(matches_3)\n",
    "    to_remove.extend(matches_4)\n",
    "    to_remove.extend(matches_5)\n",
    "    temp = [i for i in temp if not i in to_remove]\n",
    "    ### get rid of 1 and 2 letter words AND words with 20 characters or more:\n",
    "    temp = [i for i in temp if (len(i) > 2) and (len(i) < 20) ]\n",
    "    ### Stem the words (e.g. using and used becomes use)\n",
    "    stemmer = PorterStemmer()\n",
    "    temp = [stemmer.stem(i) for i in temp]\n",
    "    new_corpus.append(str(temp))\n",
    "\n",
    "    bad_col_4 = re.compile(\"xlink*\")\n",
    "    bad_col_5 = re.compile(\"label*\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    " ##### Step 2: Brake the article list into list of lists of articles:\n",
    "import math\n",
    "\n",
    "####### Calculate 'chunk_num' equal chunks and the remainder:\n",
    "length = len(new_corpus)\n",
    "size = math.floor(length / chunk_num)\n",
    "remainder = length%chunk_num\n",
    "\n",
    "####### Get an equal size chunk list of your corpus list (i.e. a list of a list):\n",
    "corpus_chunks = {}\n",
    "for i in range(chunk_num):\n",
    "    temp_list = []\n",
    "    for j in range(((size+(size*i)) - size), size+(size*i)):\n",
    "        if (size+(size*i)) <= length:\n",
    "            temp_list.append(new_corpus[j])\n",
    "            corpus_chunks[i] = temp_list\n",
    "\n",
    "####### Get the remainder of the list:\n",
    "temp_list = []\n",
    "for j in range(size*chunk_num, length):\n",
    "    temp_list.append(new_corpus[j])\n",
    "    corpus_chunks[chunk_num] = temp_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    " ##### Step 3: Make a method to create a 'bag of words' (all unique words in the corpus and # of words in each article):\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def make_matrix(corpus):\n",
    "    matrix = []\n",
    "    # Create a list of all the words in this chunk of the corpus:\n",
    "    unique_words = []\n",
    "    unique_words = list(set(\" \".join(corpus).split()))\n",
    "    for i in range(len(corpus)):\n",
    "        # Get all unique words across all articles in this chunk:\n",
    "        bbb = []\n",
    "        bbb = corpus[i].split()\n",
    "        counter = Counter(bbb)\n",
    "        # Turn the dictionary into a matrix row using the vocab.\n",
    "        row = [counter.get(w, 0) for w in unique_words]\n",
    "        matrix.append(row)\n",
    "    df = pd.DataFrame(matrix)\n",
    "    # Make the column names the words\n",
    "    df.columns = unique_words\n",
    "    # Exclude columns with less than n words metioned in this chunk\n",
    "    df.drop([col for col, val in df.sum().iteritems() if val < 3], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    " ##### Step 4: Use the main method from step 3 on each chunk of the corpus to get a list of 'bag of words' dataframes:\n",
    "dataframe_collection = {}\n",
    "for i in range(len(corpus_chunks)):\n",
    "    dataframe_collection[i] = make_matrix(corpus_chunks[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################# \n",
    " ##### Step 5: Create a dataframe with all words across lists. This allows you to:\n",
    "    # Create a column sum dataframe (i.e. the sum of each word across the entire corpus).\n",
    "    # Remove columns with under n words in this entire chunk to reduce the size of the dataframe:\n",
    "\n",
    "####################################\n",
    " ## Step 5a: This makes ALL dataframes in the list have the same columns (and in the same order):\n",
    "       # NOTE FOR MODIFICATIONS: If needed, this will allow you to move the dataframe to SQL or some other program, union and create the TFIDF from there.\n",
    "########################\n",
    "####### This loop gets all the columns from all dataframes into one list:\n",
    "full_col_list = []\n",
    "for i in range(len(dataframe_collection)):\n",
    "    temp = []\n",
    "    temp = list(dataframe_collection[i].columns)\n",
    "    full_col_list = full_col_list + temp\n",
    "\n",
    "####### This makes this list unique (turning it into a set then back to a list):\n",
    "full_col_list = list(set(full_col_list))\n",
    "\n",
    "\n",
    "####### This makes an empty dataframe with all the columns from all dataframes 'full_col_list'\n",
    "import pandas as pd\n",
    "main_df = pd.DataFrame(columns=full_col_list)\n",
    "\n",
    "\n",
    "####### This concats the main_df dataframe (with all the columns) with each dataframe. It also turns all NaN into 0\n",
    "  # Most important, this means all dataframes will have the same columns in the same order:\n",
    "modified_collection = {}\n",
    "for i in range(len(dataframe_collection)):\n",
    "    modified_collection[i] = pd.concat([main_df, dataframe_collection[i]], ignore_index=True, sort=False)\n",
    "    modified_collection[i] = modified_collection[i].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    " ## Step 5b: Use column sums of each dataframe to remove columns with under n words in total:\n",
    "########################\n",
    "\n",
    "#####################################\n",
    " ##### 5b1. Create a dataframe where each row is the column sum of each dataframe.\n",
    "df_colsum = pd.DataFrame(columns=full_col_list)\n",
    "for i in range(len(modified_collection)):\n",
    "    temp = []\n",
    "    temp = modified_collection[i].sum(axis=0)\n",
    "    df_colsum = df_colsum.append(temp,ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    " ##### 5b2. Sum the col sums across dataframes and remove all columns with under n words from ALL dataframes.\n",
    "   ## I use 5 words here (for 1/4th of allofplos & only title/abstract, probably use a much higher number for full text).\n",
    "      ## For the FULL allofplos, I should then use words with under 40 or 50 counts)\n",
    "n_words = 5\n",
    "\n",
    "all_col_sum = df_colsum.sum(axis=0)\n",
    "all_col_sum = all_col_sum[all_col_sum < n_words]    # Doing the .sum(axis=0) transposes the dataframe and makes it a series.\n",
    "all_col_sum = list(all_col_sum.index)\n",
    "\n",
    "\n",
    "####### This is your full word sum (with words under n dropped)\n",
    "df_colsum = df_colsum.drop(all_col_sum, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####### This is your full bag of words with ALL words (as a list of dataframes (with words under n dropped):\n",
    "for i in range(len(modified_collection)):\n",
    "    modified_collection[i] = modified_collection[i].drop(all_col_sum, axis=1)\n",
    "\n",
    "\n",
    "####### This is your full bag of words with only words for each iteration to save disk space (as a list of dataframes (with words under n dropped):\n",
    "for i in range(len(dataframe_collection)):\n",
    "    df_cols = list(dataframe_collection[i].columns)\n",
    "    for j in all_col_sum:\n",
    "        if j in df_cols:\n",
    "            dataframe_collection[i] = dataframe_collection[i].drop(j, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################# \n",
    " ##### Step 6. Add in rownames of the DOIs (either in dataframe_collection or modified_collection depending on which you want to use)\n",
    "\n",
    "################### DISABLED FOR NOW:\n",
    "####### For modified_collection: Use modified_collection if you want every dataframe to have ALL words in the corpus as columns (i.e. same columns for all dataframe)\n",
    "#count = 0\n",
    "#for i in range(len(modified_collection)):\n",
    "#    modified_collection[i].index = research_article_list[count : (count+len(modified_collection[i]))]\n",
    "#    count = count + len(modified_collection[i])\n",
    "\n",
    "####### For dataframe_collection\n",
    "count = 0\n",
    "for i in range(len(dataframe_collection)):\n",
    "    dataframe_collection[i].index = research_article_list[count : (count+len(dataframe_collection[i]))]\n",
    "    count = count + len(dataframe_collection[i])\n",
    "\n",
    "\n",
    "\n",
    "###### Combined with above, this allows you to time how fast your code runs:\n",
    "print(\"My program took\", time.time() - start_time, \"to run\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    " ####################################################################\n",
    "  ########## Step 7: Stop here and download datasets\n",
    "     ### 'modified_collection' here is the dataframe list you should save for future use (since TF-IDF will change over time but this bag of words list of dataframes will remain constant).\n",
    " ####################################################################\n",
    "#########################################################################\n",
    "\n",
    "################################################################# \n",
    " ##### Step 7: Export list of data frames 'modified_collection' for future use.\n",
    "################################################################# \n",
    "\n",
    "##########################\n",
    "####### Step 7a: Export the chunks of bags of words:\n",
    "##########################\n",
    "\n",
    "###################    *** This is disable for now ***\n",
    "####### 7a1: modified_collection contains All words in (and same columns for) each dataframe: Print out the list of words for each DOI (for future article connections)\n",
    "  ## NOTE: Because it's a list of dataframes, you'll need to save as n different csvs:\n",
    "#for i in range(len(modified_collection)):\n",
    "#    modified_collection[i].to_csv(rf\"TFIDFs_dataset_{i}.csv\")\n",
    "\n",
    "####### 7a2: USE THIS ONLY TO SAVE SPACE: Only words that exist in each respective dataframe: Print out the list of words for each DOI (for future article connections)\n",
    "for i in range((len(dataframe_collection))):\n",
    "    count = i + 80\n",
    "    dataframe_collection[i].to_csv(rf\"allofplos_bagofwords/smaller_TFIDFs_dataset_{count}.csv\")\n",
    "\n",
    "\n",
    "\n",
    "##########################\n",
    "####### Step 7b: Export the sum of each word in the corpus:\n",
    "##########################\n",
    "full_word_count = pd.DataFrame(df_colsum.sum(axis=0)).T\n",
    "full_word_count.to_csv(rf\"Summed_allofplos_BagOfWords_5.csv\")\n",
    "      ### NOTE: If you do a huge file, write the CSV as a zip file by using: full_word_count.to_csv(rf\"full_BagOfWordszipped.csv\", compression='gzip')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    " ####################################################################\n",
    "  ########## Step 8: Combining different iterations and removing more unwanted words.\n",
    "     ### AFTER CREATING ALL CSVS: Import and modify all bag of words and column sums\n",
    " ####################################################################\n",
    "######################################################################### \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "####### Imports sum of all words for the 5 iterations:\n",
    "import_sum_1 = pd.read_csv(\"Summed_allofplos_BagOfWords.csv\")\n",
    "import_sum_2 = pd.read_csv(\"Summed_allofplos_BagOfWords_2.csv\")\n",
    "import_sum_3 = pd.read_csv(\"Summed_allofplos_BagOfWords_3.csv\")\n",
    "import_sum_4 = pd.read_csv(\"Summed_allofplos_BagOfWords_4.csv\")\n",
    "import_sum_5 = pd.read_csv(\"Summed_allofplos_BagOfWords_5.csv\")\n",
    "\n",
    "####### Sum the sum of all words across the 5 iterations\n",
    "full_word_count = pd.DataFrame()\n",
    "full_word_count = pd.concat([import_sum_1, import_sum_2], ignore_index=True, sort=False)\n",
    "full_word_count = pd.concat([full_word_count, import_sum_3], ignore_index=True, sort=False)\n",
    "full_word_count = pd.concat([full_word_count, import_sum_4], ignore_index=True, sort=False)\n",
    "full_word_count = pd.concat([full_word_count, import_sum_5], ignore_index=True, sort=False)\n",
    "full_word_count = full_word_count.fillna(0)\n",
    "full_word_count = pd.DataFrame(full_word_count.sum(axis=0))\n",
    "\n",
    "\n",
    "####### Drop columns with a count of 25 & under:\n",
    "cols_to_drop = full_word_count[full_word_count.values <= 25]\n",
    "cols_to_drop = cols_to_drop.index\n",
    "cols_to_drop = list(set(cols_to_drop)) # De-duplicate.\n",
    "full_word_count = full_word_count.T\n",
    "full_word_count = full_word_count.drop(cols_to_drop, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################\n",
    "########## Use this to find common words you want removed from all of the bag_of_words\n",
    "  #### ID DESIRED, TAKE THIS CODE CHUNK AND USE IN A DIFFERENT CELL:\n",
    "######################################################\n",
    "################################################################### DISABLED FOR NOW:\n",
    "#col_names = full_word_count.columns\n",
    "#g = full_word_count.sort_values(by=[col_names[0]], ascending=False)\n",
    "#g[0:50] # I went until 2k, 50 at a time. \n",
    "\n",
    "####### Use the below to remove new unwanted common words (I got these by look through the common words (see just above) and selecting the ones I wanted removed):\n",
    "#to_remove = [\"'plos,'\", \"'use',\", \"'using',\", \"'abstract',\", \"'sec',\", \"'title',\", \"'italic',\", \"'summary',\", \"'type',\", \"'toc',\", \"'bold',\", \"'relatedarticle',\", \"'page',\", \"'relatedarticletype',\", \"'article-title',\", \"'sup',\", \"'abstracttype',\", \"'reftypebibr',\", \"'xref',\", \"'contenttypepdf',\", \"'ref',\", \"'idsec',\", \"'summarytitl',\", \"'pthe',\", \"'yearyear',\", \"'abstracttypesummari',\", \"'volumevolum',\", \"'xlinktypesimplenam',\", \"'mixedcit',\", \"'rowspan',\", \"'xlinktypesimpl',\", \"'italicin',\", \"'extlinktypeuri',\", \"'titleresultstitl',\", \"'titlemethodstitl',\", \"'fpagefpag',\", \"'abstracttypetoc',\", \"'italicsital',\", \"'lpagelpag',\", \"'cdsupsup',\", \"'id',\", \"'titlemethod',\", \"'metaanalysi',\", \"'sourceplo',\", \"'namedcont',\", \"'articletitleth',\", \"'mimetypeimag',\", \"'commentdoiextlink',\", \"'italicmital',\", \"'fpageefpag',\", \"'abstractsec',\", \"'titleresultstitl',\", \"'findingstitl',\", \"'titlemethodstitl',\", \"'italicpitaliclt',\", \"'titlemethod',\", \"'pthese',\", \"'iii',\", \"'persongroup',\", \"'tgfβ',\", \"'ip',\", \"'extlink']',\", \"'extlink',\", \"'fpagefpag',\", \"'mimetypeimag',\", \"'lpagelpag',\", \"'cellsp']',\", \"'cellsp',\", \"'secsec',\", \"'titleresultstitl',\", \"'titlemethodstitl',\", \"'italicpital',\", \"'sectypehead',\", \"'link',\", \"'sec',\", \"'titleobjectivetitl',\", \"'tlr',\", \"'findingstitl',\", \"'ital',\", \"'supplement',\", \"'namedcont',\", \"'italica',\", \"'titlemethod',\", \"'methodstitl',\", \"'italicsital',\", \"'titlepurposetitl',\", \"'italicd',\", \"'titlemateri',\", \"'italicnital',\", \"'italict',\", \"'italicnamedcont',\", \"'italich',\", \"'titletri',\", \"'italiceital',\", \"'contenttypegenu',\", \"'titleresultstitl',\", \"'titlemethodstitl',\", \"'italicpital',\", \"'xlinktypesimpl',\", \"'titleobjectivetitl',\", \"'italicsital',\", \"'italiccital',\", \"'italiceital',\", \"'subtyp',\", \"'italicaital',\", \"'italicbital',\", \"'italicnital',\", \"'titlepurposetitl',\", \"'titlemateri',\", \"'italiccoliital',\", \"'italiclital',\", \"'titleresultstitl',\", \"'titlemethodstitl',\", \"'italicpital',\", \"'xlinktypesimpl',\"]\n",
    "#to_remove = list(set(to_remove)) # De-duplicate.\n",
    "   # NOTE: The columns end up having a ' then a ,' around them. SO you either add in ' and ,' for the words in to_remove OR remove those punctuations from the column names.\n",
    "\n",
    "#aa = list(full_word_count.columns)\n",
    "#for i in to_remove:\n",
    "#    if i in aa:\n",
    "#        full_word_count = full_word_count.drop(i, axis=1)\n",
    "\n",
    "full_word_count.to_csv(r\"Full_allofplos_bagofwords.csv\")\n",
    "   # After this, you can delete all 4 'Summed_allofplos_BagOfWords' iterations and just use this as your full summed bag_of_words.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####### Import and drop columns with a count under 25 (and commons words you want removed), then (re)save these files:\n",
    "path = \"allofplos_bagofwords/\"\n",
    "fileList = os.listdir(path)\n",
    "temp = {}\n",
    "for i in range(len(fileList)):\n",
    "    temp[i] = pd.read_csv(\"allofplos_bagofwords/\"+fileList[i])\n",
    "    dois = temp[i].iloc[:,0]\n",
    "    bb = list(temp[i].columns)\n",
    "    for j in cols_to_drop:\n",
    "        if j in bb:\n",
    "            temp[i] = temp[i].drop(j, axis=1)\n",
    "#    for j in to_remove:                       ############# DISABLED FOR NOW: ENABLE IF YOU ADD TO THE 'TO_REMOVE' LIST\n",
    "#        if j in bb:\n",
    "#            temp[i] = temp[i].drop(j, axis=1)\n",
    "    temp[i].index = dois\n",
    "    temp[i].to_csv(rf\"final_allofplos_bagofwords/smaller_TFIDFs_dataset_{i}.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#########################################################################\n",
    "####################################################################\n",
    " ###### See file 'TFIDF_new_recommendations' for doing TF-IDFs, doing the actual recommendations, and adding in new files.\n",
    "####################################################################\n",
    "#########################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyPy3",
   "language": "python",
   "name": "pypy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
