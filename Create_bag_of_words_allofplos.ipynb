{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################\n",
    "    #####################################################################################################\n",
    "             ############  Text proprocessing, article by article (bag of words)  ############\n",
    "    #####################################################################################################\n",
    "###############################################################################################################\n",
    "\n",
    "############################################################\n",
    " ##### TO DO FIRST TO MAKE THIS WORK: \n",
    "############################################################\n",
    "\n",
    "########## 1. Change all xml files into txt: \n",
    " # *** Go to the command prompt and type in:\n",
    "   # A. cd\\\n",
    "   # B. cd\\Users\\jthibodeaux\\python\\Lib\\site-packages\\allofplos\\allofplos_text\n",
    "   # C. ren *.xml *.txt\n",
    "\n",
    "\n",
    "############################################################\n",
    "##########  TO DO SECOND TO MAKE THIS WORK: \n",
    "  ## This code reaches memory failure after about 70k articles for title and abstract only. \n",
    "  ## SO: Break allofplos corpus into 1/4ths \n",
    "       # Break into even smaller fractions if you're doing the entire text instead of title/abstract).\n",
    "############################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    " ##### The below opens / reads all of the text files from a folder into Python.\n",
    "#####################################\n",
    "import os\n",
    "import pandas as pd\n",
    "allLines = []\n",
    "path = 'python/Lib/site-packages/allofplos/allofplos_text_5/'\n",
    "  # NOTE: This means the 'allofplos_text' folder has to be a child folder of the folder this jupyter notebook is in.\n",
    "fileList = os.listdir(path)\n",
    "for file in fileList:\n",
    "   file = open(os.path.join('python/Lib/site-packages/allofplos/allofplos_text_5/'+ file), 'r', encoding='utf-8')\n",
    "   allLines.append(file.read())\n",
    "\n",
    "\n",
    "####### This changes the file list into dois by taking off the .txt:\n",
    "fileList = [x[:-4] for x in fileList]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    " ##### Specify how many chunks you want this iteration of your corpus broken further into:\n",
    "    # This is to prevent memory failure AND to provide datasets with few columns (and thus a lower storage size).\n",
    "    # Smaller chunks = larger bag_of_words file sizes BUT slower processing of TF-IDFs and related articles. \n",
    "#####################################\n",
    "chunk_num = 18\n",
    "\n",
    "\n",
    "\n",
    "#### Make sure all files were imported by checking length:\n",
    "len(allLines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    " ##### BOTH OF THE BELOW FILTERS BY RESEARCH-ARTICLES *AND* TAKES OUT HTML LANGUAGE\n",
    "#################################################\n",
    "\n",
    "##############################\n",
    " ## Time difference:\n",
    "    ##  #1 (ALL WORDS) takes about 3 articles per second. #2 (title & abstract only) takes about 30 articles per second. \n",
    "##############################\n",
    "\n",
    "\n",
    "#################################################\n",
    "##### 1. The below use ALL WORDS from the article:\n",
    "#################################################\n",
    "\n",
    "############################################################################ Disabled for now\n",
    "#import re\n",
    "#from bs4 import BeautifulSoup\n",
    "#full_corpus = []\n",
    "#research_article_list = []\n",
    "#for i in range(0, len(allLines)):\n",
    "#    if len(re.findall('article-type=\"research-article', allLines[i])) > 0:    \n",
    "#        soup = BeautifulSoup(allLines[i], \"xml\")\n",
    "#        text = soup.get_text(strip=False)\n",
    "#        full_corpus.append(text)\n",
    "#        research_article_list.append(fileList[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################################\n",
    "##### 2. The below removes all text except title and abstract:\n",
    "#################################################\n",
    "\n",
    "##################\n",
    "######## This allows you to get the title and abstract: \n",
    "#### NOTE: Using <abstract> xml tag works for most but not all; the below 3 methods gets them all:\n",
    "#### You use split() to split the data by a particular string, then only use the string beforehand.\n",
    "##Title\n",
    "   # Start: <title-group>\n",
    "   # End: <alt-title alt-title-type\n",
    "##Abstract_1:\n",
    "   # Start: </permissions>\n",
    "   # End: </abstract>\n",
    "      # Also, add on a second group of text if there's over 1 </abstract> after the </permissions>\n",
    "##Abstract_2:\n",
    "   # Start: </permissions>\n",
    "   # End: <abstract abstract-type=\"summary\">\n",
    "##################\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "full_corpus = []\n",
    "research_article_list = []\n",
    "for i in range(0, len(allLines)):\n",
    "    if len(re.findall('article-type=\"research-article', allLines[i])) > 0:    \n",
    "        text = str(allLines[i])\n",
    "        ## Titles look to be called similarly across allofplos (abstracts are more difficult, see below)\n",
    "        title = text.split('<alt-title alt-title-type')[0]\n",
    "        title = title.split('<title-group>')[1]\n",
    "        ## This is an additional way to include abstracts BUT they aren't in every article:\n",
    "        Another_abstract = 0\n",
    "        if len(re.findall('</abstract></article-meta>', text)) > 0:\n",
    "            abstract_2 = text.split('</abstract></article-meta>')[0]\n",
    "            abstract_2 = abstract_2.split('</permissions>')[1]\n",
    "            Another_abstract = Another_abstract + 1\n",
    "        else:         # This is the main way to get the abstract:\n",
    "            abstract = text.split('</permissions>')[1]\n",
    "            abstract_1 = abstract.split('</abstract>')[0]\n",
    "        ### Get rid of xml tags for the title :\n",
    "             ## NOTE: In many cases, using BeautifulSoup on the abstract portion removes a lot of the abstract so I removed it (I remove related words in the below cell under 'to_remove' such as 'abstract'):\n",
    "        soup = BeautifulSoup(title, \"xml\")\n",
    "        title = soup.get_text(strip=False)\n",
    "        ### If there's more than one </abstract> after permissions, add the part after that first </abstract> too.\n",
    "           ## NOTE: included title twice to highlight it's importance.\n",
    "        if Another_abstract > 0:\n",
    "            text = title + \" \" + title + \" \" + abstract_2\n",
    "        else:\n",
    "            if len(re.findall('</abstract>', abstract)) > 1:\n",
    "                abstract_3 = abstract.split('</abstract>')[1]\n",
    "                text = title + \" \" + title + \" \" + abstract_1 + \" \" + abstract_3\n",
    "            else:\n",
    "                text = title + \" \" + title + \" \" + abstract_1\n",
    "        full_corpus.append(text)\n",
    "        research_article_list.append(fileList[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Combined with below, this allows you to time how fast your code runs:\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#####################################\n",
    " ##### Step 1: Create a loop to clean the articles (e.g. punctuation, upper cases, etc):\n",
    "#####################################\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "## The following 2 could be skipped if you want to keep the stopwords:\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "new_corpus = []\n",
    "for i in range(len(full_corpus)):\n",
    "    temp = []\n",
    "    temp = full_corpus[i].lower()\n",
    "    temp = re.sub(r'\\d+', '', temp)\n",
    "    temp = temp.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    temp = temp.strip()\n",
    "    ### The following three could be skipped if you want to keep the stop words:\n",
    "    temp = word_tokenize(temp)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    temp = [i for i in temp if not i in stop_words]\n",
    "    ### Get rid of any words you think should be removed (eg uninformative common words and tags)\n",
    "    to_remove = [\"plos\", \"use\", \"using\", \"abstract\", \"sec\", \"title\", \"italic\", \"summary\", \"type\", \"toc\", \"bold\", \"relatedarticle\", \"page\", \"relatedarticletype\", \"article-title\", \"sup\", \"abstracttype\", \"reftypebibr\", \"xref\", \"contenttypepdf\", \"ref\", \"idsec\", \"summarytitl\", \"pthe\", \"yearyear\", \"abstracttypesummari\", \"volumevolum\", \"xlinktypesimplenam\", \"mixedcit\", \"rowspan\", \"xlinktypesimpl\", \"italicin\", \"extlinktypeuri\", \"titleresultstitl\", \"titlemethodstitl\", \"fpagefpag\", \"abstracttypetoc\", \"italicsital\", \"lpagelpag\", \"cdsupsup\", \"id\", \"titlemethod\", \"metaanalysi\", \"sourceplo\", \"namedcont\", \"articletitleth\", \"mimetypeimag\", \"commentdoiextlink\", \"italicmital\", \"fpageefpag\", \"abstractsec\", \"titleresultstitl\", \"findingstitl\", \"titlemethodstitl\", \"italicpitaliclt\", \"titlemethod\", \"pthese\", \"iii\", \"persongroup\", \"tgfβ\", \"ip\", \"extlink']\", \"extlink\", \"fpagefpag\", \"mimetypeimag\", \"lpagelpag\", \"cellsp']\", \"cellsp\", \"secsec\", \"titleresultstitl\", \"titlemethodstitl\", \"italicpital\", \"sectypehead\", \"link\", \"sec\", \"titleobjectivetitl\", \"tlr\", \"findingstitl\", \"ital\", \"supplement\", \"namedcont\", \"italica\", \"titlemethod\", \"methodstitl\", \"italicsital\", \"titlepurposetitl\", \"italicd\", \"titlemateri\", \"italicnital\", \"italict\", \"italicnamedcont\", \"italich\", \"titletri\", \"italiceital\", \"contenttypegenu\", \"titleresultstitl\", \"titlemethodstitl\", \"italicpital\", \"xlinktypesimpl\", \"titleobjectivetitl\", \"italicsital\", \"italiccital\", \"italiceital\", \"subtyp\", \"italicaital\", \"italicbital\", \"italicnital\", \"titlepurposetitl\", \"titlemateri\", \"italiccoliital\", \"italiclital\", \"italicpital\", \"unique\", \"name\", \"use\", \"would\", \"either\", \"introduction\"]\n",
    "    ## bad_col: Get rid of all words with 'X' in them (since beautiful soup doesn't work well on abstracts):\n",
    "    bad_col_1 = re.compile(\"link:*\")                       # NOTE: This matching on link bit is new.\n",
    "    bad_col_2 = re.compile(\"abstract*\")\n",
    "    bad_col_3 = re.compile(\"title*\")\n",
    "    bad_col_4 = re.compile(\"xlink*\")\n",
    "    bad_col_5 = re.compile(\"label*\")\n",
    "    matches_1 = [i for i in temp if re.match(bad_col_1, i)]\n",
    "    matches_2 = [i for i in temp if re.match(bad_col_2, i)]\n",
    "    matches_3 = [i for i in temp if re.match(bad_col_3, i)]\n",
    "    matches_4 = [i for i in temp if re.match(bad_col_4, i)]\n",
    "    matches_5 = [i for i in temp if re.match(bad_col_5, i)]\n",
    "    to_remove.extend(matches_1)\n",
    "    to_remove.extend(matches_2)\n",
    "    to_remove.extend(matches_3)\n",
    "    to_remove.extend(matches_4)\n",
    "    to_remove.extend(matches_5)\n",
    "    temp = [i for i in temp if not i in to_remove]\n",
    "    ### get rid of 1 and 2 letter words AND words with 20 characters or more:\n",
    "    temp = [i for i in temp if (len(i) > 2) and (len(i) < 20) ]\n",
    "    ### Stem the words (e.g. using and used becomes use)\n",
    "    stemmer = PorterStemmer()\n",
    "    temp = [stemmer.stem(i) for i in temp]\n",
    "    new_corpus.append(str(temp))\n",
    "    bad_col_4 = re.compile(\"xlink*\")\n",
    "    bad_col_5 = re.compile(\"label*\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    " ##### Step 2: Brake the article list into chunks (list of lists) of articles:\n",
    "#####################################\n",
    "import math\n",
    "\n",
    "####### Calculate 'chunk_num' equal chunks and the remainder:\n",
    "length = len(new_corpus)\n",
    "size = math.floor(length / chunk_num)\n",
    "remainder = length%chunk_num\n",
    "\n",
    "####### Get an equal size chunk list of your corpus list (i.e. a list of a list):\n",
    "corpus_chunks = {}\n",
    "for i in range(chunk_num):\n",
    "    temp_list = []\n",
    "    for j in range(((size+(size*i)) - size), size+(size*i)):\n",
    "        if (size+(size*i)) <= length:\n",
    "            temp_list.append(new_corpus[j])\n",
    "            corpus_chunks[i] = temp_list\n",
    "\n",
    "####### Get the remainder of the list:\n",
    "temp_list = []\n",
    "for j in range(size*chunk_num, length):\n",
    "    temp_list.append(new_corpus[j])\n",
    "    corpus_chunks[chunk_num] = temp_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    " ##### Step 3: Method to create a 'bag of words' (all unique words in the corpus and # of words in each article)\n",
    "#####################################\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def make_matrix(corpus):\n",
    "    matrix = []\n",
    "    # Create a list of all the words in this chunk of the corpus:\n",
    "    unique_words = []\n",
    "    unique_words = list(set(\" \".join(corpus).split()))\n",
    "    for i in range(len(corpus)):\n",
    "        # Get all unique words across all articles in this chunk:\n",
    "        bbb = []\n",
    "        bbb = corpus[i].split()\n",
    "        counter = Counter(bbb)\n",
    "        # Turn the dictionary into a matrix row using the vocab.\n",
    "        row = [counter.get(w, 0) for w in unique_words]\n",
    "        matrix.append(row)\n",
    "    df = pd.DataFrame(matrix)\n",
    "    # Make the column names the words\n",
    "    df.columns = unique_words\n",
    "    # Exclude columns with less than n words metioned in this chunk\n",
    "    df.drop([col for col, val in df.sum().iteritems() if val < 3], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    " ##### Step 4: Use the main method from step 3 on each chunk of the corpus to get a list of 'bag of words' dataframes:\n",
    "#####################################\n",
    "dataframe_collection = {}\n",
    "for i in range(len(corpus_chunks)):\n",
    "    dataframe_collection[i] = make_matrix(corpus_chunks[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################# \n",
    " ##### Step 5: Create a dataframe with all words across lists. This allows you to:\n",
    "    # A: Create a column sum dataframe (i.e. the sum of each word across the entire corpus).\n",
    "    # B: Remove columns with under n words in this entire chunk to reduce the size of the dataframe:\n",
    "################################################################# \n",
    "####################################\n",
    " ## Step 5a: This makes ALL dataframes in the list have the same columns (and in the same order):\n",
    "       # NOTE FOR MODIFICATIONS: If needed, this will allow you to move the dataframe to SQL or some other program, union and create the TFIDF from there.\n",
    "####################################\n",
    "####### This loop gets all the columns (i.e. words) from all dataframes into one list:\n",
    "full_col_list = []\n",
    "for i in range(len(dataframe_collection)):\n",
    "    temp = []\n",
    "    temp = list(dataframe_collection[i].columns)\n",
    "    full_col_list = full_col_list + temp\n",
    "\n",
    "####### Make this list unique:\n",
    "full_col_list = list(set(full_col_list))\n",
    "\n",
    "\n",
    "####### Make an empty dataframe with all the columns (i.e. words) from all dataframes 'full_col_list'\n",
    "import pandas as pd\n",
    "main_df = pd.DataFrame(columns=full_col_list)\n",
    "\n",
    "\n",
    "####### This concats the main_df dataframe (with all the columns) with each dataframe. It also turns all NaN into 0\n",
    "  # Most important, this means all dataframes will have the same columns in the same order:\n",
    "modified_collection = {}\n",
    "for i in range(len(dataframe_collection)):\n",
    "    modified_collection[i] = pd.concat([main_df, dataframe_collection[i]], ignore_index=True, sort=False)\n",
    "    modified_collection[i] = modified_collection[i].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    " ## Step 5b: Use column sums of each dataframe to remove columns with under n words in total:\n",
    "####################################\n",
    "\n",
    "##############################\n",
    " ##### 5b1. Create a dataframe where each row is the column sum of each dataframe.\n",
    "##############################\n",
    "df_colsum = pd.DataFrame(columns=full_col_list)\n",
    "for i in range(len(modified_collection)):\n",
    "    temp = []\n",
    "    temp = modified_collection[i].sum(axis=0)\n",
    "    df_colsum = df_colsum.append(temp,ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    " ##### 5b2. Sum the col sums across dataframes and remove all columns with under n words from ALL dataframes.\n",
    "   ## I use 5 words here (for 1/4th of allofplos & only title/abstract, probably use a much higher number for full text).\n",
    "      ## In Step 7 I will remove words with a higher number after all iterations are combined.\n",
    "##############################\n",
    "n_words = 5\n",
    "\n",
    "all_col_sum = df_colsum.sum(axis=0)\n",
    "all_col_sum = all_col_sum[all_col_sum < n_words]\n",
    "all_col_sum = list(all_col_sum.index)\n",
    "\n",
    "\n",
    "####### This is your full word sum (with words under n dropped)\n",
    "df_colsum = df_colsum.drop(all_col_sum, axis=1)\n",
    "\n",
    "\n",
    "####### This is your full bag of words with ALL words (as a list of dataframes (with words under n dropped):\n",
    "for i in range(len(modified_collection)):\n",
    "    modified_collection[i] = modified_collection[i].drop(all_col_sum, axis=1)\n",
    "\n",
    "\n",
    "####### This is your full bag of words with only words for each chunk and words under 5 dropped (saves disk space):\n",
    "for i in range(len(dataframe_collection)):\n",
    "    df_cols = list(dataframe_collection[i].columns)\n",
    "    for j in all_col_sum:\n",
    "        if j in df_cols:\n",
    "            dataframe_collection[i] = dataframe_collection[i].drop(j, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################# \n",
    " ##### Step 6. Add in rownames of the DOIs (either in dataframe_collection or modified_collection depending on which you want to use)\n",
    "################################################################# \n",
    "\n",
    "################### DISABLED FOR NOW:\n",
    "####### For modified_collection: Use modified_collection if you want every dataframe to have ALL words in the corpus as columns (i.e. same columns for all dataframe)\n",
    "#count = 0\n",
    "#for i in range(len(modified_collection)):\n",
    "#    modified_collection[i].index = research_article_list[count : (count+len(modified_collection[i]))]\n",
    "#    count = count + len(modified_collection[i])\n",
    "\n",
    "####### For dataframe_collection (bag_of_words with only words for each chunk)\n",
    "count = 0\n",
    "for i in range(len(dataframe_collection)):\n",
    "    dataframe_collection[i].index = research_article_list[count : (count+len(dataframe_collection[i]))]\n",
    "    count = count + len(dataframe_collection[i])\n",
    "\n",
    "\n",
    "\n",
    "###### Combined with above, this allows you to time how fast your code runs:\n",
    "print(\"My program took\", time.time() - start_time, \"to run\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    " ####################################################################\n",
    "  ########## Step 7: Stop here and download datasets\n",
    "     ### 'modified_collection' here is the dataframe list you should save for future use (since TF-IDF will change over time but this bag of words list of dataframes will remain constant).\n",
    " ####################################################################\n",
    "#########################################################################\n",
    "\n",
    "################################################################# \n",
    " ##### Step 7: Export list of data frames 'modified_collection' for future use.\n",
    "################################################################# \n",
    "\n",
    "##########################\n",
    "####### Step 7a: Export the chunks of bags of words:\n",
    "##########################\n",
    "\n",
    "###################    *** This is disable for now ***\n",
    "####### 7a1: Modified_collection contains All words in (and same columns for) each dataframe: Save to file\n",
    "  ## NOTE: Because it's a list of dataframes, you'll need to save as n different csvs:\n",
    "#for i in range(len(modified_collection)):\n",
    "#    modified_collection[i].to_csv(rf\"TFIDFs_dataset_{i}.csv\")\n",
    "\n",
    "####### 7a2: ALMOST ALWAYS USE THIS: Only columns (words) that exist in each respective chunk: Save to file\n",
    "for i in range((len(dataframe_collection))):\n",
    "    count = i + 80\n",
    "    dataframe_collection[i].to_csv(rf\"allofplos_bagofwords/smaller_TFIDFs_dataset_{count}.csv\")\n",
    "\n",
    "\n",
    "\n",
    "##########################\n",
    "####### Step 7b: Export the sum of each word in the corpus:\n",
    "##########################\n",
    "full_word_count = pd.DataFrame(df_colsum.sum(axis=0)).T\n",
    "full_word_count.to_csv(rf\"Summed_allofplos_BagOfWords_5.csv\")\n",
    "      ### NOTE: If you do a huge file, write the CSV as a zip file by using: full_word_count.to_csv(rf\"full_BagOfWordszipped.csv\", compression='gzip')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################################################################\n",
    " ###### Do the above for as many sets of files you broke the entire corpus into\n",
    "   # NOTE: I broke it into 5 sets of around 55k files each for title x2 and abstract\n",
    " ###### When done with those do the below (step 8)\n",
    "####################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    " ####################################################################\n",
    "  ########## Step 8: Combining different iterations and removing more unwanted words.\n",
    "     ### AFTER CREATING ALL CSVS: Import and modify all bag of words and column sums\n",
    " ####################################################################\n",
    "######################################################################### \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "####### Imports sum of all words for the 5 iterations:\n",
    "import_sum_1 = pd.read_csv(\"Summed_allofplos_BagOfWords.csv\")\n",
    "import_sum_2 = pd.read_csv(\"Summed_allofplos_BagOfWords_2.csv\")\n",
    "import_sum_3 = pd.read_csv(\"Summed_allofplos_BagOfWords_3.csv\")\n",
    "import_sum_4 = pd.read_csv(\"Summed_allofplos_BagOfWords_4.csv\")\n",
    "import_sum_5 = pd.read_csv(\"Summed_allofplos_BagOfWords_5.csv\")\n",
    "\n",
    "####### Sum the sum of all words across the 5 iterations\n",
    "full_word_count = pd.DataFrame()\n",
    "full_word_count = pd.concat([import_sum_1, import_sum_2], ignore_index=True, sort=False)\n",
    "full_word_count = pd.concat([full_word_count, import_sum_3], ignore_index=True, sort=False)\n",
    "full_word_count = pd.concat([full_word_count, import_sum_4], ignore_index=True, sort=False)\n",
    "full_word_count = pd.concat([full_word_count, import_sum_5], ignore_index=True, sort=False)\n",
    "full_word_count = full_word_count.fillna(0)\n",
    "full_word_count = pd.DataFrame(full_word_count.sum(axis=0))\n",
    "\n",
    "\n",
    "####### Drop columns with a count of n & under (I eliminated words used under 25 times total in the entire corpus):\n",
    "cols_to_drop = full_word_count[full_word_count.values <= 25]\n",
    "cols_to_drop = cols_to_drop.index\n",
    "cols_to_drop = list(set(cols_to_drop)) # De-duplicate.\n",
    "full_word_count = full_word_count.T\n",
    "full_word_count = full_word_count.drop(cols_to_drop, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################\n",
    "########## OPTIONAL/MESSY: Use this to find common words you want removed from all of the bag_of_words\n",
    "  #### IF DESIRED, TAKE THIS CODE CHUNK AND USE IN A DIFFERENT CELL:\n",
    "######################################################\n",
    "################################################################### DISABLED FOR NOW:\n",
    "#col_names = full_word_count.columns\n",
    "#g = full_word_count.sort_values(by=[col_names[0]], ascending=False)\n",
    "#g[0:50] # I went until 2k, 50 at a time. \n",
    "\n",
    "####### Use the below to remove new unwanted common words (I got these by looking through the common words (see just above) and selecting the ones I wanted removed):\n",
    "#to_remove = [\"'plos,'\", \"'use',\", \"'using',\", \"'abstract',\", \"'sec',\", \"'title',\", \"'italic',\", \"'summary',\", \"'type',\", \"'toc',\", \"'bold',\", \"'relatedarticle',\", \"'page',\", \"'relatedarticletype',\", \"'article-title',\", \"'sup',\", \"'abstracttype',\", \"'reftypebibr',\", \"'xref',\", \"'contenttypepdf',\", \"'ref',\", \"'idsec',\", \"'summarytitl',\", \"'pthe',\", \"'yearyear',\", \"'abstracttypesummari',\", \"'volumevolum',\", \"'xlinktypesimplenam',\", \"'mixedcit',\", \"'rowspan',\", \"'xlinktypesimpl',\", \"'italicin',\", \"'extlinktypeuri',\", \"'titleresultstitl',\", \"'titlemethodstitl',\", \"'fpagefpag',\", \"'abstracttypetoc',\", \"'italicsital',\", \"'lpagelpag',\", \"'cdsupsup',\", \"'id',\", \"'titlemethod',\", \"'metaanalysi',\", \"'sourceplo',\", \"'namedcont',\", \"'articletitleth',\", \"'mimetypeimag',\", \"'commentdoiextlink',\", \"'italicmital',\", \"'fpageefpag',\", \"'abstractsec',\", \"'titleresultstitl',\", \"'findingstitl',\", \"'titlemethodstitl',\", \"'italicpitaliclt',\", \"'titlemethod',\", \"'pthese',\", \"'iii',\", \"'persongroup',\", \"'tgfβ',\", \"'ip',\", \"'extlink']',\", \"'extlink',\", \"'fpagefpag',\", \"'mimetypeimag',\", \"'lpagelpag',\", \"'cellsp']',\", \"'cellsp',\", \"'secsec',\", \"'titleresultstitl',\", \"'titlemethodstitl',\", \"'italicpital',\", \"'sectypehead',\", \"'link',\", \"'sec',\", \"'titleobjectivetitl',\", \"'tlr',\", \"'findingstitl',\", \"'ital',\", \"'supplement',\", \"'namedcont',\", \"'italica',\", \"'titlemethod',\", \"'methodstitl',\", \"'italicsital',\", \"'titlepurposetitl',\", \"'italicd',\", \"'titlemateri',\", \"'italicnital',\", \"'italict',\", \"'italicnamedcont',\", \"'italich',\", \"'titletri',\", \"'italiceital',\", \"'contenttypegenu',\", \"'titleresultstitl',\", \"'titlemethodstitl',\", \"'italicpital',\", \"'xlinktypesimpl',\", \"'titleobjectivetitl',\", \"'italicsital',\", \"'italiccital',\", \"'italiceital',\", \"'subtyp',\", \"'italicaital',\", \"'italicbital',\", \"'italicnital',\", \"'titlepurposetitl',\", \"'titlemateri',\", \"'italiccoliital',\", \"'italiclital',\", \"'titleresultstitl',\", \"'titlemethodstitl',\", \"'italicpital',\", \"'xlinktypesimpl',\"]\n",
    "#to_remove = list(set(to_remove)) # De-duplicate.\n",
    "   # NOTE: The columns end up having a ' then a ,' around them. SO you either add in ' and ,' for the words in to_remove OR remove those punctuations from the column names.\n",
    "\n",
    "#aa = list(full_word_count.columns)\n",
    "#for i in to_remove:\n",
    "#    if i in aa:\n",
    "#        full_word_count = full_word_count.drop(i, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############## Save a file full word count across the entire corpus to file:\n",
    "full_word_count.to_csv(r\"Full_allofplos_bagofwords.csv\")\n",
    "   # After this, you can delete all 4 'Summed_allofplos_BagOfWords' iterations and just use this as your full summed bag_of_words.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############## Import and drop columns with a count under n words (and commons words you want removed), then (re)save these files:\n",
    "path = \"allofplos_bagofwords/\"\n",
    "fileList = os.listdir(path)\n",
    "temp = {}\n",
    "for i in range(len(fileList)):\n",
    "    temp[i] = pd.read_csv(\"allofplos_bagofwords/\"+fileList[i])\n",
    "    dois = temp[i].iloc[:,0]\n",
    "    bb = list(temp[i].columns)\n",
    "    for j in cols_to_drop:\n",
    "        if j in bb:\n",
    "            temp[i] = temp[i].drop(j, axis=1)\n",
    "#    for j in to_remove:                    ############# DISABLED FOR NOW: ENABLE IF YOU ADD THE 'TO_REMOVE' LIST\n",
    "#        if j in bb:\n",
    "#            temp[i] = temp[i].drop(j, axis=1)\n",
    "    temp[i].index = dois\n",
    "    temp[i].to_csv(rf\"final_allofplos_bagofwords/smaller_TFIDFs_dataset_{i}.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "###############################################################\n",
    " ###### See file 'Create_related_article_lists_for_allofplos' for doing TF-IDFs & related articles.\n",
    "###############################################################\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyPy3",
   "language": "python",
   "name": "pypy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
