{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "  ############ Creating TF-IDFs and making related article lists across allofplos ############ \n",
    "######################################################################################################\n",
    "\n",
    "#####################################\n",
    " ##### The below opens / reads all of the bag_of_words csvs from a folder into Python.\n",
    "#####################################\n",
    "import os\n",
    "import pandas as pd\n",
    "path = \"final_allofplos_bagofwords/\"\n",
    "fileList = os.listdir(path)\n",
    "bag_of_words = {}\n",
    "dois = []\n",
    "old_doi_list = []\n",
    "for i in range(len(fileList)):\n",
    "    bag_of_words[i] = pd.read_csv(\"final_allofplos_bagofwords/\"+fileList[i])\n",
    "    dois = bag_of_words[i].iloc[:,0]\n",
    "    bag_of_words[i].index = dois\n",
    "    old_doi_list.extend(dois)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################ \n",
    " ##### Use column sums of each dataframe to get final TF-IDF:\n",
    "############################################################ \n",
    "\n",
    "###### Take out columns with strings in the values (would only be in first few columns but I went to 100 just in case):\n",
    "  ## The index also has the DOI name so use index. \n",
    "for i in range(len(bag_of_words)):\n",
    "    for j in range(100):\n",
    "        if type(bag_of_words[i].iloc[1,j]) is str:\n",
    "            bag_of_words[i] = bag_of_words[i].drop(bag_of_words[i].columns[j], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    " ##### 1 Get TF part of TF-IDF (values divided by their row sums)\n",
    "for i in range(len(bag_of_words)):\n",
    "    row_sum = bag_of_words[i].sum(axis=1)\n",
    "    bag_of_words[i] = bag_of_words[i] / row_sum[:,None]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#####################################\n",
    " ##### 2. Get IDF for all dataframes. \n",
    "##### Get IDF (column sums / sum of all words):\n",
    "full_col_sum = pd.read_csv(\"Full_allofplos_BagOfWords.csv\")\n",
    "\n",
    "### Take out columns with the name 'Unnamed: X' (would only be in first column but I went up to 100 just in case):\n",
    "import re\n",
    "bad_col = re.compile(\"Unnamed:*\")\n",
    "for j in range(100):\n",
    "    if len(bad_col.findall(full_col_sum.columns[j])) > 0:\n",
    "        full_col_sum = full_col_sum.drop(full_col_sum.columns[j], axis=1)\n",
    "\n",
    "### Transpose, get values, then sum.\n",
    "sum_transposed = full_col_sum.T\n",
    "sum_all_words = int(sum(sum_transposed.values))\n",
    "\n",
    "idf = full_col_sum / sum_all_words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    " ##### 3. Get TF-IDF for all dataframes: \n",
    "####### Divide all 'bag_of_words' (which now are each value divided by it's row sum) by it's column's IDF:\n",
    "#####################################\n",
    "for i in range(len(bag_of_words)):\n",
    "    ## Left join bag_of_words onto the full list of words: The makes the final row the corpus word count:\n",
    "    bag_of_words[i] = bag_of_words[i].T\n",
    "    trans_idf = idf.T\n",
    "    bag_of_words[i] = bag_of_words[i].join(trans_idf).T\n",
    "    ## Get TF-IDF for all rows; except final row (which is the sum of words in this loop)\n",
    "       ## NOTE: This loop went quicker than the non-loop option: bag_of_words[i] = bag_of_words[i] / idf\n",
    "    for j in range(len(bag_of_words[i].columns)):\n",
    "        bag_of_words[i].iloc[:,j] = bag_of_words[i].iloc[:,j] / bag_of_words[i].iloc[(len(bag_of_words[i].index)-1),j]\n",
    "    ## Drop the row with column sums:\n",
    "    bag_of_words[i].drop(bag_of_words[i].tail(1).index,inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Combined with below, this allows you to time how fast your code runs:\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "##########################################\n",
    " ## MAIN: Get top 19 DOIS by TF-IDF correlation between DOI being compared and all other DOIs in corpus.\n",
    "##########################################\n",
    "##########################################\n",
    " ## Notes on time:\n",
    "    ## Takes around 6 seconds per DOI (meaning the whole corpus takes a long time).\n",
    "    ## You can break the DOIs to compare into chunks and do the program simultaneously across different computers.\n",
    "    ## Also, it's set up to allow for starting and stopping: See the top of the loop.\n",
    "##########################################\n",
    "\n",
    "\n",
    "recommendations = 20\n",
    "\n",
    "final_data = pd.DataFrame(columns = range(recommendations))\n",
    "\n",
    "######################### I STOPPED AT THE BELOW (CAN JUST START IT UP, IT ACTUALLY STARTS AT THE FILE CALLED '12' BECAUSE IT GOES 0,1,10,11,12, ETC)\n",
    "\n",
    "for iter_dataset in range(0, len(bag_of_words)-1):     ############ If you need to stop and start, change this line's range accordingly (e.g. if you went up to only chunk 15, you can start the program again starting at range(15,len(bag_of_words)-1)\n",
    "    for iter_doi in range(len(bag_of_words[iter_dataset])):\n",
    "        aa = pd.DataFrame(bag_of_words[iter_dataset].iloc[iter_doi,:].T)\n",
    "        # Reduce DOI to compare data to words that exist in that DOI:\n",
    "        aa = aa.fillna(0)\n",
    "        aa = aa[aa.values > 0]\n",
    "        #Get dataframe of only TF-IDFs in DOI being compared over 20. \n",
    "        x =  aa[aa.values > 20]\n",
    "        corrs = pd.DataFrame()\n",
    "        # This loop gets the correlations of the DOI for each bag_of_words:\n",
    "        for i in range(0, (len(bag_of_words)-1)):\n",
    "            new = bag_of_words[i]\n",
    "            # If 'new' is the same dataframe as the one that has the DOI being compared, remove the DOI being compared from 'new'\n",
    "            if new.index[0] == bag_of_words[iter_dataset].index[0]:\n",
    "                new = new.drop([aa.columns[0]], axis = 0)\n",
    "            #To increase speed, remove DOIs who have a summed TF-IDF of under 100 for TF-IDF over 5 in the DOI being compared.\n",
    "            b = x.join(new.T)\n",
    "            c = b.sum(axis=0)\n",
    "            c = list(c[c.values < 1000].index)\n",
    "            new = new.drop(c, axis=0)\n",
    "            b = pd.concat([new, aa.T], sort=False)\n",
    "            b = b.fillna(0).T\n",
    "            #Do a corelation of the first dataframe with the second:\n",
    "            c = pd.DataFrame(b[b.columns[0:]].corr()[b.columns[len(b.columns)-1]])[:-1]\n",
    "            #Cut down to the number of recommendation and format to prepare for final data:\n",
    "            corrs = pd.concat([corrs, c], sort=False)\n",
    "            corrs = corrs.sort_values(corrs.columns[0], ascending=False).head(n = recommendations)\n",
    "        to_compare = [corrs.columns[0]]\n",
    "        corrs = pd.DataFrame(corrs.index)\n",
    "        corrs.columns = to_compare\n",
    "        corrs = corrs.T\n",
    "        final_data = pd.concat([final_data, corrs], sort=False)\n",
    "        ## NOTE: I put this in the loop becasue this takes so long, enables starting and stopping.\n",
    "        final_data.to_csv(rf\"RelatedDoiList_Best_Correlations_ALL.csv\")    ############ If you stop and start, use a different file new (or else you'll overwrite the last one with an empty dataframe)\n",
    "\n",
    "final_data.index = pd.Index(final_data.index, name = 'DOI Being Compared')   # use if index is the column of the compared DOI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################\n",
    " ###### See file 'Update_related_article_list' for updating related article lists.\n",
    "###############################################################\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyPy3",
   "language": "python",
   "name": "pypy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
