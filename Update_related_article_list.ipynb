{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "  ############ Updating bags_of_words, full word sums, and related articles lists ############ \n",
    "######################################################################################################\n",
    "\n",
    "#####################################\n",
    " ##### TO DO FIRST TO MAKE THIS WORK: \n",
    "#####################################\n",
    "########## 1. This is how you update allofplos: \n",
    " # A. In powershell use: python -m allofplos.update\n",
    " # B. Not needed but runs faster: In the 'allofplos_xml' folder, sort by date modified and put all new files into the folder 'allofplos_temp'\n",
    "########## 2. Change all xml files into txt: \n",
    " # *** Go to the command prompt and type in:\n",
    "   # A. cd\\\n",
    "   # B. cd\\Users\\jthibodeaux\\python\\Lib\\site-packages\\allofplos\\allofplos_temp\n",
    "   # C. ren *.xml *.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    " ##### The below opens / reads all of the bag_of_words csvs from a folder into Python.\n",
    "#####################################\n",
    "import os\n",
    "import pandas as pd\n",
    "path = \"final_allofplos_bagofwords/\"\n",
    "fileList = os.listdir(path)\n",
    "bag_of_words = {}\n",
    "dois = []\n",
    "old_doi_list = []\n",
    "for i in range(len(fileList)):\n",
    "    bag_of_words[i] = pd.read_csv(\"final_allofplos_bagofwords/\"+fileList[i])\n",
    "    dois = bag_of_words[i].iloc[:,0]\n",
    "    bag_of_words[i].index = dois\n",
    "    old_doi_list.extend(dois)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    " ##### The below opens / reads all of the text files from a folder into Python.\n",
    "#####################################\n",
    "allLines = []\n",
    "path = 'python/Lib/site-packages/allofplos/allofplos_temp/'\n",
    "  # NOTE: This means the 'allofplos_temp' folder has to be a child folder of the folder of this file.\n",
    "fileList = os.listdir(path)\n",
    "\n",
    "####### Check if a new doi already exists in the old doi list. If so, then remove:\n",
    "old_files = [i + \".txt\" for i in old_doi_list]     # Add on the .txt to the old dois.\n",
    "for i in old_files:\n",
    "    if i in fileList:\n",
    "        fileList.remove(i)\n",
    "\n",
    "####### Create the new list of text:\n",
    "for file in fileList:\n",
    "   file = open(os.path.join('python/Lib/site-packages/allofplos/allofplos_temp/'+ file), 'r', encoding='utf-8')\n",
    "   allLines.append(file.read())\n",
    "\n",
    "\n",
    "####### This changes the file list into dois by taking off the .txt:\n",
    "fileList = [x[:-4] for x in fileList]\n",
    "\n",
    "\n",
    "#### Length of new file list (minus DOIs already in the existing bag_of_words):\n",
    "len(allLines)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    " ##### BOTH OF THE BELOW FILTERS BY RESEARCH-ARTICLES *AND* TAKES OUT HTML LANGUAGE\n",
    "   ## NOTE: One reason I only get research articles is because NON-research articles (or some subset of them) have some weird issue with them when tokenizing\n",
    "#################################################\n",
    "\n",
    "#################################################\n",
    "##### 1. The below use ALL WORDS from the article:\n",
    "#################################################\n",
    "############################################################################ Disabled for now\n",
    "#import re\n",
    "#from bs4 import BeautifulSoup\n",
    "#full_corpus = []\n",
    "#research_article_list = []\n",
    "#for i in range(0, len(allLines)):\n",
    "#    if len(re.findall('article-type=\"research-article', allLines[i])) > 0:    \n",
    "#        soup = BeautifulSoup(allLines[i], \"xml\")\n",
    "#        text = soup.get_text(strip=False)\n",
    "#        full_corpus.append(text)\n",
    "#        research_article_list.append(fileList[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################################\n",
    "##### 2. The below removes all text except title and abstract:\n",
    "#################################################\n",
    "\n",
    "##################\n",
    "######## This allows you to get the title and abstract: \n",
    "#### You use split to split the data by a particular string, then only use the string beforehand.\n",
    "  # E.g. see: https://stackoverflow.com/questions/27387415/how-would-i-get-everything-before-a-in-a-string-python\n",
    "##Title\n",
    "   # Start: <title-group>\n",
    "   # End: <alt-title alt-title-type\n",
    "##Abstract_1:\n",
    "   # Start: </permissions>\n",
    "   # End: </abstract>\n",
    "      # Also, add on a second group of text if there's over 1 </abstract> after the </permissions>\n",
    "##Abstract_2:\n",
    "   # Start: <abstract abstract-type=\"summary\">\n",
    "   # End: </abstract>\n",
    "##################\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "full_corpus = []\n",
    "research_article_list = []\n",
    "for i in range(0, len(allLines)):\n",
    "    if len(re.findall('article-type=\"research-article', allLines[i])) > 0:    \n",
    "        text = str(allLines[i])\n",
    "        ## Titles look to be called similarly across allofplos (abstracts are more difficult, see below)\n",
    "        title = text.split('<alt-title alt-title-type')[0]\n",
    "        title = title.split('<title-group>')[1]\n",
    "        ## This is an additional way to include abstracts BUT they aren't in every article:\n",
    "        Another_abstract = 0\n",
    "        if len(re.findall('</abstract></article-meta>', text)) > 0:\n",
    "            abstract_2 = text.split('</abstract></article-meta>')[0]\n",
    "            abstract_2 = abstract_2.split('</permissions>')[1]\n",
    "            Another_abstract = Another_abstract + 1\n",
    "        else:         # This is the main way to get the abstract:\n",
    "            abstract = text.split('</permissions>')[1]\n",
    "            abstract_1 = abstract.split('</abstract>')[0]\n",
    "        ### Get rid of xml tags for the title :\n",
    "             ## NOTE: In many cases, using BeautifulSoup on the abstract portion removes a lot of the abstract so I disable it here (I remove related words in the below cell such as 'abstract':\n",
    "        soup = BeautifulSoup(title, \"xml\")\n",
    "        title = soup.get_text(strip=False)\n",
    "        ### If there's more than one </abstract> after permissions, add the part after that first </abstract> too.\n",
    "        if Another_abstract > 0:\n",
    "            text = title + \" \" + abstract_2\n",
    "        else:\n",
    "            if len(re.findall('</abstract>', abstract)) > 1:   ## If there's more than one </abstract> after permissions, add the part after that first </abstract> too.\n",
    "                abstract_3 = abstract.split('</abstract>')[1]\n",
    "                text = title + \" \" + abstract_1 + \" \" + abstract_3\n",
    "            else:\n",
    "                text = title + \" \" + abstract_1\n",
    "        full_corpus.append(text)\n",
    "        research_article_list.append(fileList[i])\n",
    "\n",
    "\n",
    "#### Length of new file list (minus DOIs already in the existing bag_of_words AND only including 'Research Articles'):\n",
    "len(full_corpus)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    " ##### Step 1: Create a loop to clean the articles (e.g. punctuation, upper cases) before you put it in the main method\n",
    "  ## Make the string all lower case, take out numbers, take out punctuation, take out white spaces:\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "## The following 2 could be skipped if you want to keep the stopwords:\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "new_corpus = []\n",
    "for i in range(len(full_corpus)):\n",
    "    temp = []\n",
    "    temp = full_corpus[i].lower()\n",
    "    temp = re.sub(r'\\d+', '', temp)\n",
    "    temp = temp.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    temp = temp.strip()\n",
    "    ### The following three could be skipped if you want to keep the stop words:\n",
    "    temp = word_tokenize(temp)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    temp = [i for i in temp if not i in stop_words]\n",
    "    ### Get rid of any words you think should be removed (eg uninformative common words and tags)\n",
    "    to_remove = [\"plos\", \"use\", \"using\", \"abstract\", \"sec\", \"title\", \"italic\", \"summary\", \"type\", \"toc\", \"bold\", \"relatedarticle\", \"page\", \"relatedarticletype\", \"article-title\", \"sup\", \"abstracttype\", \"reftypebibr\", \"xref\", \"contenttypepdf\", \"ref\", \"idsec\", \"summarytitl\", \"pthe\", \"yearyear\", \"abstracttypesummari\", \"volumevolum\", \"xlinktypesimplenam\", \"mixedcit\", \"rowspan\", \"xlinktypesimpl\", \"italicin\", \"extlinktypeuri\", \"titleresultstitl\", \"titlemethodstitl\", \"fpagefpag\", \"abstracttypetoc\", \"italicsital\", \"lpagelpag\", \"cdsupsup\", \"id\", \"titlemethod\", \"metaanalysi\", \"sourceplo\", \"namedcont\", \"articletitleth\", \"mimetypeimag\", \"commentdoiextlink\", \"italicmital\", \"fpageefpag\", \"abstractsec\", \"titleresultstitl\", \"findingstitl\", \"titlemethodstitl\", \"italicpitaliclt\", \"titlemethod\", \"pthese\", \"iii\", \"persongroup\", \"tgfβ\", \"ip\", \"extlink']\", \"extlink\", \"fpagefpag\", \"mimetypeimag\", \"lpagelpag\", \"cellsp']\", \"cellsp\", \"secsec\", \"titleresultstitl\", \"titlemethodstitl\", \"italicpital\", \"sectypehead\", \"link\", \"sec\", \"titleobjectivetitl\", \"tlr\", \"findingstitl\", \"ital\", \"supplement\", \"namedcont\", \"italica\", \"titlemethod\", \"methodstitl\", \"italicsital\", \"titlepurposetitl\", \"italicd\", \"titlemateri\", \"italicnital\", \"italict\", \"italicnamedcont\", \"italich\", \"titletri\", \"italiceital\", \"contenttypegenu\", \"titleresultstitl\", \"titlemethodstitl\", \"italicpital\", \"xlinktypesimpl\", \"titleobjectivetitl\", \"italicsital\", \"italiccital\", \"italiceital\", \"subtyp\", \"italicaital\", \"italicbital\", \"italicnital\", \"titlepurposetitl\", \"titlemateri\", \"italiccoliital\", \"italiclital\", \"italicpital\", \"unique\", \"name\", \"use\", \"would\", \"either\", \"introduction\"]\n",
    "    ## Get rid of all words with 'link in them' (since beautiful soup doesn't work well on abstract):\n",
    "    ## Get rid of all words with 'link in them' (since beautiful soup doesn't work well on abstract):\n",
    "    bad_col_1 = re.compile(\"link:*\")                       # NOTE: This matching on link bit is new.\n",
    "    bad_col_2 = re.compile(\"abstract*\")\n",
    "    bad_col_3 = re.compile(\"title*\")\n",
    "    bad_col_4 = re.compile(\"xlink*\")\n",
    "    bad_col_5 = re.compile(\"label*\")\n",
    "    matches_1 = [i for i in temp if re.match(bad_col_1, i)]\n",
    "    matches_2 = [i for i in temp if re.match(bad_col_2, i)]\n",
    "    matches_3 = [i for i in temp if re.match(bad_col_3, i)]\n",
    "    matches_4 = [i for i in temp if re.match(bad_col_4, i)]\n",
    "    matches_5 = [i for i in temp if re.match(bad_col_5, i)]\n",
    "    to_remove.extend(matches_1)\n",
    "    to_remove.extend(matches_2)\n",
    "    to_remove.extend(matches_3)\n",
    "    to_remove.extend(matches_4)\n",
    "    to_remove.extend(matches_5)\n",
    "    temp = [i for i in temp if not i in to_remove]\n",
    "    ### get rid of 1 and 2 letter words AND words with 20 characters or more:\n",
    "    temp = [i for i in temp if (len(i) > 2) and (len(i) < 20) ]\n",
    "    ### Stem the words (e.g. using and used becomes use)\n",
    "    stemmer = PorterStemmer()\n",
    "    temp = [stemmer.stem(i) for i in temp]\n",
    "    new_corpus.append(str(temp))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    " ##### Step 2: Make a method to create a 'bag of words' (all unique words in the corpus and # of words in each article):\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def make_matrix(corpus):\n",
    "    matrix = []\n",
    "    # Create a list of all the words in this chunk of the corpus:\n",
    "    unique_words = []\n",
    "    unique_words = list(set(\" \".join(corpus).split()))\n",
    "    for i in range(len(corpus)):\n",
    "        # Get all unique words across all articles in this chunk:\n",
    "        bbb = []\n",
    "        bbb = corpus[i].split()\n",
    "        counter = Counter(bbb)\n",
    "        # Turn the dictionary into a matrix row using the vocab.\n",
    "        row = [counter.get(w, 0) for w in unique_words]\n",
    "        matrix.append(row)\n",
    "    df = pd.DataFrame(matrix)\n",
    "    # Make the column names the words\n",
    "    df.columns = unique_words\n",
    "    # Exclude columns with less than 3 words metioned in this chunk\n",
    "    df.drop([col for col, val in df.sum().iteritems() if val < 3], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "new_corpus = make_matrix(new_corpus)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    " ##### Step 3: Bring in old bag_of_words sum of words in the entire corpus; update it and write to file:\n",
    "#####################################\n",
    "old_sum_columns = pd.read_csv(\"Full_allofplos_BagOfWords.csv\")\n",
    "new_sum_columns = new_corpus.sum(axis=0)\n",
    "\n",
    "####### Left join the two dataframes:\n",
    "tran_old_sum_columns = old_sum_columns.T\n",
    "new_sum_columns = pd.DataFrame(new_corpus.sum(axis=0))\n",
    "full_word_count = tran_old_sum_columns.join(new_sum_columns, lsuffix='_a', rsuffix='_b').T\n",
    "\n",
    "####### Prepare dataset (fill NAs, sum all rows) and export as the revised main sum of all words: \"Full_allofplos_bagofwords.csv\"\n",
    "full_word_count = full_word_count.fillna(0)\n",
    "full_word_count = full_word_count.sum(axis=0)\n",
    "full_word_count = pd.DataFrame(full_word_count)\n",
    "full_word_count = full_word_count.T\n",
    "full_word_count.to_csv(r\"Full_allofplos_bagofwords.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    " ##### Step 4: Add in rownames of the DOIs and add this bag_of_words to the big list of files:\n",
    "#####################################\n",
    "new_corpus = new_corpus.fillna(0)\n",
    "new_corpus.index = research_article_list\n",
    "#new_corpus.insert(0, \"DOI to compare\", research_article_list)\n",
    "\n",
    "####### Add new bag of words to the rest of the csvs, then add onto the bag_of_words list of lists:\n",
    "count = (len(bag_of_words))\n",
    "new_corpus.to_csv(rf\"final_allofplos_bagofwords/smaller_TFIDFs_dataset_{count}.csv\")\n",
    "\n",
    "bag_of_words[count] = new_corpus\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################# \n",
    " ##### Step 5: Use column sums of each dataframe to get final TF-IDF:\n",
    "################################################################# \n",
    "\n",
    "####### First you need to take out the 1st column \"Unnamed: 0\" or 'DOI to compare', which contains the DOIs.\n",
    "  # The index also has the DOI name so use index. \n",
    "###### Take out columns with strings in the values (would only be in first few columns but I went to 100 just in case):\n",
    "for i in range(len(bag_of_words)):\n",
    "    for j in range(100):\n",
    "        if type(bag_of_words[i].iloc[1,j]) is str:\n",
    "            bag_of_words[i] = bag_of_words[i].drop(bag_of_words[i].columns[j], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    " ##### 5a. Get TF part of TF-IDF (values divided by their row sums)\n",
    "for i in range(len(bag_of_words)):\n",
    "    row_sum = bag_of_words[i].sum(axis=1)\n",
    "    bag_of_words[i] = bag_of_words[i] / row_sum[:,None]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################\n",
    " ##### 5b. Get IDF for all dataframes. \n",
    "##### Get IDF (column sums / sum of all words):\n",
    "full_col_sum = pd.read_csv(\"Full_allofplos_BagOfWords.csv\")\n",
    "\n",
    "### Take out columns with the name 'Unnamed: X' (would only be in first few columns but I went up to 100 just in case):\n",
    "import re\n",
    "bad_col = re.compile(\"Unnamed:*\")\n",
    "for j in range(100):\n",
    "    if len(bad_col.findall(full_col_sum.columns[j])) > 0:\n",
    "        full_col_sum = full_col_sum.drop(full_col_sum.columns[j], axis=1)\n",
    "\n",
    "\n",
    "### Transpose, get values, then sum.\n",
    "sum_transposed = full_col_sum.T\n",
    "sum_all_words = int(sum(sum_transposed.values))\n",
    "\n",
    "idf = full_col_sum / sum_all_words\n",
    "\n",
    "\n",
    "#####################################\n",
    " ##### 5c. Get TF-IDF for all dataframes: \n",
    "####### Divide all 'bag_of_words' (which is now TF: each value divided by it's row sum) by it's column's IDF:\n",
    "#####################################\n",
    "for i in range(len(bag_of_words)):\n",
    "    ## Left join bag_of_words onto the full list of words: The makes the final row the corpus word count:\n",
    "    bag_of_words[i] = bag_of_words[i].T\n",
    "    trans_idf = idf.T\n",
    "    bag_of_words[i] = bag_of_words[i].join(trans_idf).T\n",
    "    ## Get TF-IDF for all rows; except final row (which is the sum of words in this loop)\n",
    "       ## NOTE: This loop went quicker than the non-loop option: bag_of_words[i] = bag_of_words[i] / idf\n",
    "    for j in range(len(bag_of_words[i].columns)):\n",
    "        bag_of_words[i].iloc[:,j] = bag_of_words[i].iloc[:,j] / bag_of_words[i].iloc[(len(bag_of_words[i].index)-1),j]\n",
    "    ## Drop the row with column sums:\n",
    "    bag_of_words[i].drop(bag_of_words[i].tail(1).index,inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################# \n",
    " ##### Step 6: Use the TF-IDF to create one large dataframe where each DOI has n most relevant articles (in order)\n",
    "################################################################# \n",
    "\n",
    "###### Combined with below, this allows you to time how fast your code runs:\n",
    "import pandas as pd\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "##########################################\n",
    " ## MAIN:  Get top 19 DOIS by TF-IDF correlation between DOI being compared and all other DOIs in corpus.\n",
    "    ### The below assumes the new bag_of_words is the final one in the list (which it's set up as in Step 4)\n",
    "##########################################\n",
    "\n",
    "recommendations = 20\n",
    "\n",
    "final_data = pd.DataFrame(columns = range(recommendations))\n",
    "\n",
    "for iter_doi in range(len(bag_of_words[(len(bag_of_words)-1)])):\n",
    "    aa = pd.DataFrame(bag_of_words[(len(bag_of_words)-1)].iloc[iter_doi,:].T)\n",
    "    # Reduce DOI to compare data to words that exist in that DOI:\n",
    "    aa = aa.fillna(0)\n",
    "    aa = aa[aa.values > 0]\n",
    "    #Get dataframe of only TF-IDFs in DOI being compared over 20. \n",
    "    x =  aa[aa.values > 20]\n",
    "    corrs = pd.DataFrame()\n",
    "    # This loop gets the correlations of the DOI for each bag_of_words:\n",
    "    for i in range(0, (len(bag_of_words)-1)):\n",
    "        new = bag_of_words[i]\n",
    "        # If 'new' is the same dataframe as the one that has the DOI being compared, remove the DOI being compared from 'new'\n",
    "        if new.index[0] == bag_of_words[len(bag_of_words)-1].index[0]:\n",
    "            new = new.drop([aa.columns[0]], axis = 0)\n",
    "        #To increase speed, remove DOIs who have a summed TF-IDF of under 100 for TF-IDF over 5 in the DOI being compared.\n",
    "        b = x.join(new.T)\n",
    "        c = b.sum(axis=0)\n",
    "        c = list(c[c.values < 1000].index)\n",
    "        new = new.drop(c, axis=0)\n",
    "        b = pd.concat([new, aa.T], sort=False)\n",
    "        b = b.fillna(0).T\n",
    "        #Do a corelation of the first dataframe with the second:\n",
    "        c = pd.DataFrame(b[b.columns[0:]].corr()[b.columns[len(b.columns)-1]])[:-1]\n",
    "        #Cut down to the number of recommendation and format to prepare for final data:\n",
    "        corrs = pd.concat([corrs, c], sort=False)\n",
    "        corrs = corrs.sort_values(corrs.columns[0], ascending=False).head(n = recommendations)\n",
    "    to_compare = [corrs.columns[0]]\n",
    "    corrs = pd.DataFrame(corrs.index)\n",
    "    corrs.columns = to_compare\n",
    "    corrs = corrs.T\n",
    "    final_data = pd.concat([final_data, corrs], sort=False)\n",
    "    ## NOTE: I put this in the loop becasue this takes so long, enables starting and stopping.\n",
    "    final_data.to_csv(rf\"RelatedDoiList_Best_Correlations.csv\")\n",
    "\n",
    "final_data.index = pd.Index(final_data.index, name = 'DOI Being Compared')   # use if index is the column of the compared DOI\n",
    "\n",
    "\n",
    "###### Combined with above, this allows you to time how fast your code runs:\n",
    "print(\"My program took\", time.time() - start_time, \"to run\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################# \n",
    " ##### Step 7: The below inserts new related DOIs into the existing related dois list:\n",
    "    ## E.g. DOI X is the 5th most related article for a new DOI Y. Using the below, DOI X will now have DOI Y as their 5th most related article, the existing 5th through 2nd to last related articles are moved down, and the least related article in the list is removed.\n",
    "    ## NOTE: It is now time optimized within the loop by only searching through rows in the old list that will be changed for each new DOI.\n",
    "      ## Takes about 0.8 seconds per new DOI\n",
    "#################################################################\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "######## This is where I bring in the existing DOI related articles list and new DOI related articles list.\n",
    "  ## If these are already in the memory of the Jupyter notebook you could change the code or for convenience just do: new = final_data.copy()\n",
    "import pandas as pd\n",
    "existing = pd.read_csv(\"RelatedDoiList_Best_Correlations_ALL.csv\")\n",
    "new = pd.read_csv(\"RelatedDoiList_Best_Correlations.csv\")\n",
    "#### Or import new from the cell above:\n",
    "#new = final_data.copy()\n",
    "\n",
    "######## Attach new to existing first (so new DOIs can be updated with new DOI matches too):\n",
    "new = new.rename(columns={new.columns[0]: \"DOI Being Compared\"})\n",
    "existing = pd.concat([existing, new], sort = False)\n",
    "\n",
    "\n",
    "######## This is the main loop to insert new DOIs into the old:\n",
    "for new_row in range(len(new)):\n",
    "    ###### Subset for each new_col and new_row to significantly speed up loop:\n",
    "    ## 1. Make a list of related DOIs in the current row of new:\n",
    "    new_doi_list = list(new.iloc[new_row,1:len(new.columns)])\n",
    "    rows_to_modify_list = []\n",
    "    for i in range(1, len(existing.columns)-1):   ### Skip the last column ... If need be for time, you could skip the last n columns (e.g. skipping 17-19 may save a good amount of time if you're updating over half the rows in the dataset) ... If you do so, change the line (37) with \"Skip the last column\" below similarly. \n",
    "        rows_to_modify_list.extend(list(existing[existing.iloc[:,i].isin(new_doi_list)].iloc[:,0]))\n",
    "    ## 3. Create two datasets: One with rows that will not be changed, the other with rows that will be changed:\n",
    "    unchanged_data = existing[~existing.iloc[:,0].isin(rows_to_modify_list)]\n",
    "    changed_data = existing[existing.iloc[:,0].isin(rows_to_modify_list)].copy()\n",
    "    for new_col in range(1, (len(new.columns) - 2)):  ### Skip the last column\n",
    "        for exist_row in range(len(changed_data)):\n",
    "            if new.iloc[new_row, new_col] == changed_data.iloc[exist_row, 0]:   ### NOTE: The 'existing column' is always 0 here.\n",
    "                ## 1. Replace the 2nd through last column's values with the 1st through 2nd to last column values.\n",
    "                changed_data.iloc[exist_row, (new_col+1):(len(changed_data.columns))] = list(changed_data.iloc[exist_row, new_col:(len(changed_data.columns)-1)])\n",
    "                ## 2. Replace the first value with the 1st value from the new dataframe.\n",
    "                changed_data.iloc[exist_row, new_col] = new.iloc[new_row, 0]\n",
    "    ###### Change 'existing' by concatinating changed data with unchanged data:\n",
    "    existing = pd.concat([unchanged_data, changed_data])\n",
    "\n",
    "\n",
    "######## Export to CSV file:\n",
    "existing.to_csv(r\"RelatedDoiList_Best_Correlations_ALL.csv\", index=False)\n",
    "\n",
    "print(\"My program took\", time.time() - start_time, \"to run\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyPy3",
   "language": "python",
   "name": "pypy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
